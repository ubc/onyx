# Default values for onyx.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  # Global version for all Onyx components (overrides .Chart.AppVersion)
  version: "latest"
  # Global pull policy for all Onyx component images
  pullPolicy: "IfNotPresent"
  # Host for all Onyx components
  host: "0.0.0.0"

postgresql:
  enabled: true
  # IMPORTANT: This nameOverride is required for the CloudNativePG operator to find itself.
  # The operator looks for a deployment with label app.kubernetes.io/name=cloudnative-pg,
  # but since the subchart is aliased as "postgresql", Helm defaults to that name.
  nameOverride: cloudnative-pg
  cluster:
    instances: 1
    storage:
      storageClass: ""
      size: 10Gi
    enableSuperuserAccess: true
    superuserSecret:
      name: onyx-postgresql  # keep in sync with auth.postgresql

# -- Master toggle for vector database support. When false:
#   - Sets DISABLE_VECTOR_DB=true on all backend pods
#   - Skips the indexing model server deployment (embeddings not needed)
#   - Skips docprocessing and docfetching celery workers
#   - You should also set vespa.enabled=false and opensearch.enabled=false
#     to prevent those subcharts from deploying
vectorDB:
  enabled: true

vespa:
  name: da-vespa-0
  service:
    name: vespa-service
  volumeClaimTemplates:
    - metadata:
        name: vespa-storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 30Gi
        storageClassName: ""
  enabled: true
  replicaCount: 1
  image:
    repository: vespa
    tag: "8.609.39"
  podAnnotations: {}
  podLabels:
    app: vespa
    app.kubernetes.io/instance: onyx
    app.kubernetes.io/name: vespa
  securityContext:
    privileged: true
    runAsUser: 0
  resources:
    # The Vespa Helm chart specifies default resources, which are quite modest. We override
    # them here to increase chances of the chart running successfully. If you plan to index at
    # scale, you will likely need to increase these limits further.
    # At large scale, it is recommended to use a dedicated Vespa cluster / Vespa cloud.
    requests:
      cpu: 4000m
      memory: 8000Mi
    limits:
      cpu: 8000m
      memory: 32000Mi

opensearch:
  enabled: false
  # These values are passed to the opensearch subchart.
  # See https://github.com/opensearch-project/helm-charts/blob/main/charts/opensearch/values.yaml

  singleNode: true  # Forces replicas=1, sets discovery.type=single-node

  # Determines service DNS: onyx-opensearch-master.<namespace>.svc.cluster.local
  clusterName: "onyx-opensearch"
  nodeGroup: "master"
  masterService: "onyx-opensearch-master"

  replicas: 1

  image:
    repository: "opensearchproject/opensearch"
    tag: ""  # Empty uses chart's appVersion (3.4.0).

  # The security plugin requires OPENSEARCH_INITIAL_ADMIN_PASSWORD for
  # OpenSearch 2.12+.
  # See https://docs.opensearch.org/latest/install-and-configure/install-opensearch/helm/#prerequisites
  extraEnvs:
    - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD
      valueFrom:
        secretKeyRef:
          name: onyx-opensearch  # Must match auth.opensearch.secretName.
          key: opensearch_admin_password  # Must match auth.opensearch.secretKeys value.

  resources:
    requests:
      cpu: 2000m
      memory: 4Gi
    limits:
      cpu: 4000m
      memory: 8Gi

  persistence:
    enabled: true
    size: 30Gi
    storageClass: ""

  # Java heap should be ~50% of memory limit.
  # See https://opster.com/guides/opensearch/opensearch-basics/opensearch-heap-size-usage-and-jvm-garbage-collection/
  # Xms is the starting size, Xmx is the maximum size. These should be the same.
  opensearchJavaOpts: "-Xmx4g -Xms4g"

persistent:
  storageClassName: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

autoscaling:
  # Valid options: 'hpa' (default) or 'keda'.
  # Set to 'keda' to render KEDA ScaledObjects for components that have autoscaling enabled.
  # When using KEDA you must install and manage the KEDA operator separately; it is not bundled with this chart.
  engine: hpa

inferenceCapability:
  service:
    portName: modelserver
    type: ClusterIP
    servicePort: 9000
    targetPort: 9000
  name: inference-model-server
  replicaCount: 1
  labels:
    - key: app
      value: inference-model-server
  image:
    repository: onyxdotapp/onyx-model-server
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  containerPorts:
    server: 9000
  podLabels:
    - key: app
      value: inference-model-server
  resources:
    requests:
      cpu: 2000m
      memory: 3Gi
    limits:
      cpu: 4000m
      memory: 10Gi
  # Optional health probes
  # Example:
  # readinessProbe:
  #   httpGet:
  #     path: /health
  #     port: model-server
  startupProbe: {}
  readinessProbe: {}
  livenessProbe: {}
  podSecurityContext: {}
  securityContext:
    privileged: true
    runAsUser: 0
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Deployment strategy - use Recreate or RollingUpdate with maxSurge: 0 to terminate old pod first
  # This prevents pending pods when cluster resources are constrained
  strategy: {}
  # Example for RollingUpdate that terminates old pod first:
  # strategy:
  #   type: RollingUpdate
  #   rollingUpdate:
  #     maxSurge: 0
  #     maxUnavailable: 1


indexCapability:
  service:
    portName: modelserver
    type: ClusterIP
    servicePort: 9000
    targetPort: 9000
  replicaCount: 1
  name: indexing-model-server
  deploymentLabels:
    app: indexing-model-server
  podLabels:
    scope: onyx-backend
  indexingOnly: "True"
  podAnnotations: {}
  containerPorts:
    server: 9000
  image:
    repository: onyxdotapp/onyx-model-server
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  limitConcurrency: 10
  resources:
    requests:
      cpu: 4000m
      memory: 3Gi
    limits:
      cpu: 6000m
      memory: 6Gi
  # Optional health probes
  # Example:
  # readinessProbe:
  #   httpGet:
  #     path: /health
  #     port: model-server
  startupProbe: {}
  readinessProbe: {}
  livenessProbe: {}
  podSecurityContext: {}
  securityContext:
    privileged: true
    runAsUser: 0
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Deployment strategy - use Recreate or RollingUpdate with maxSurge: 0 to terminate old pod first
  # This prevents pending pods when cluster resources are constrained
  strategy: {}
  # Example for RollingUpdate that terminates old pod first:
  # strategy:
  #   type: RollingUpdate
  #   rollingUpdate:
  #     maxSurge: 0
  #     maxUnavailable: 1
config:
  envConfigMapName: env-configmap

tooling:
  pgInto:
    # -- Mounts a small helper script into app pods that opens psql using the pod's POSTGRES_* env vars.
    enabled: false
    # -- Where to place the helper inside the container.
    mountPath: /usr/local/bin/pginto
    # -- Which client binary to call; change if your image uses a non-default path.
    psqlBinary: psql

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

nginx:
  enabled: true
  # Nginx proxy timeout settings (in seconds)
  timeouts:
    connect: 300  # Time to establish connection with upstream server
    send: 300     # Time to send request to upstream server
    read: 300     # Time to read response from upstream server
  controller:
    containerPort:
      http: 1024

    # NOTE: When onyx-nginx-conf changes, nginx pods need to restart.
    # The ingress-nginx subchart doesn't auto-detect our custom ConfigMap changes.
    # Workaround: Helm upgrade will restart if the following annotation value changes.
    podAnnotations:
      onyx.app/nginx-config-version: "1"

    # Propagate DOMAIN into nginx so server_name continues to use the same env var
    extraEnvs:
      - name: DOMAIN
        value: localhost

    config:
      # Expose DOMAIN to the nginx config and pull in our custom snippets
      main-snippet: |
        env DOMAIN;
      http-snippet: |
        include /etc/nginx/custom-snippets/upstreams.conf;
        include /etc/nginx/custom-snippets/server.conf;

    # Mount the existing nginx ConfigMap that holds the upstream and server snippets
    extraVolumes:
      - name: nginx-config
        configMap:
          name: onyx-nginx-conf
    extraVolumeMounts:
      - name: nginx-config
        mountPath: /etc/nginx/custom-snippets
        readOnly: true

    service:
      type: LoadBalancer
      ports:
        http: 80
      targetPorts:
        http: http

webserver:
  replicaCount: 1
  image:
    repository: onyxdotapp/onyx-web-server
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  deploymentLabels:
    app: web-server
  podAnnotations: {}
  podLabels:
    scope: onyx-frontend
  podSecurityContext:
    {}
    # fsGroup: 2000

  securityContext:
    {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  containerPorts:
    server: 3000

  service:
    type: ClusterIP
    servicePort: 3000
    targetPort: http

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi

  # Optional health probes
  # Example:
  # readinessProbe:
  #   httpGet:
  #     path: /api/health
  #     port: http
  startupProbe: {}
  readinessProbe: {}
  livenessProbe: {}

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    # Example: customTriggers: []
    #   - type: prometheus
    #     metadata:
    #       serverAddress: http://prometheus:9090
    #       metricName: http_requests_per_second
    #       threshold: '100'
    customTriggers: []

  # Additional volumes on the output Deployment definition.
  volumes: []
  # - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  # Additional volumeMounts on the output Deployment definition.
  volumeMounts: []
  # - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  nodeSelector: {}
  tolerations: []
  affinity: {}

api:
  replicaCount: 1
  image:
    repository: onyxdotapp/onyx-backend
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  deploymentLabels:
    app: api-server
  podAnnotations: {}
  podLabels:
    scope: onyx-backend

  containerPorts:
    server: 8080

  podSecurityContext:
    {}
    # fsGroup: 2000

  securityContext:
    {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    type: ClusterIP
    servicePort: 8080
    targetPort: api-server-port
    portName: api-server-port

  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 3Gi

  # Optional health probes
  # Example:
  # readinessProbe:
  #   httpGet:
  #     path: /health
  #     port: api-server-port
  startupProbe: {}
  readinessProbe: {}
  livenessProbe: {}

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    # Example: customTriggers: []
    #   - type: prometheus
    #     metadata:
    #       serverAddress: http://prometheus:9090
    #       metricName: http_requests_per_second
    #       threshold: '100'
    customTriggers: []

  # Additional volumes on the output Deployment definition.
  volumes: []
  # - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  # Additional volumeMounts on the output Deployment definition.
  volumeMounts: []
  # - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  nodeSelector: {}
  tolerations: []
  affinity: {}


######################################################################
#
# Background workers
#
######################################################################

celery_shared:
  image:
    repository: onyxdotapp/onyx-backend
    tag: ""  # Overrides the image tag whose default is the chart appVersion.
  startupProbe:
    # startupProbe fails after 2m
    exec:
      command: ["test", "-f", "/app/onyx/main.py"]
    failureThreshold: 24
    periodSeconds: 5
    timeoutSeconds: 3
  readinessProbe:
    # readinessProbe fails after 15s + 2m of inactivity
    # it's ok to see the readinessProbe fail transiently while the container starts
    initialDelaySeconds: 15
    periodSeconds: 5
    failureThreshold: 24
    timeoutSeconds: 3
  livenessProbe:
    # livenessProbe fails after 5m of inactivity
    initialDelaySeconds: 60
    periodSeconds: 60
    failureThreshold: 5
    timeoutSeconds: 3
  podSecurityContext: {}
  securityContext:
    privileged: true
    runAsUser: 0

celery_beat:
  replicaCount: 1
  logLevel: INFO
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
  deploymentLabels:
    app: celery-beat
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_heavy:
  replicaCount: 1
  logLevel: INFO
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
  deploymentLabels:
    app: celery-worker-heavy
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 2Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_docprocessing:
  replicaCount: 1
  logLevel: INFO
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 20
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
  deploymentLabels:
    app: celery-worker-docprocessing
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 12Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_light:
  replicaCount: 1
  logLevel: INFO
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
  deploymentLabels:
    app: celery-worker-light
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 4Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_monitoring:
  replicaCount: 1
  logLevel: INFO
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
  deploymentLabels:
    app: celery-worker-monitoring
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 4Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_primary:
  replicaCount: 1
  logLevel: INFO
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
  deploymentLabels:
    app: celery-worker-primary
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 4Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_user_file_processing:
  replicaCount: 1
  logLevel: INFO
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
  deploymentLabels:
    app: celery-worker-user-file-processing
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Discord bot for Onyx
# The bot offloads message processing to scalable API pods via HTTP requests.
discordbot:
  enabled: false  # Disabled by default - requires bot token configuration
  # Bot token can be provided directly or via a Kubernetes secret
  # Option 1: Direct token (not recommended for production)
  botToken: ""
  # Option 2: Reference a Kubernetes secret (recommended)
  botTokenSecretName: ""  # Name of the secret containing the bot token
  botTokenSecretKey: "token"  # Key within the secret (default: "token")
  # Command prefix for bot commands (default: "!")
  invokeChar: "!"
  image:
    repository: onyxdotapp/onyx-backend
    tag: ""  # Overrides the image tag whose default is the chart appVersion.
  podAnnotations: {}
  podLabels:
    scope: onyx-backend
  deploymentLabels:
    app: discord-bot
  podSecurityContext:
    {}
  securityContext:
    {}
  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "2000Mi"
  volumes: []
  volumeMounts: []
  nodeSelector: {}
  tolerations: []
  affinity: {}

slackbot:
  enabled: true
  replicaCount: 1
  image:
    repository: onyxdotapp/onyx-backend
    tag: ""  # Overrides the image tag whose default is the chart appVersion.
  podAnnotations: {}
  podLabels:
    scope: onyx-backend
  deploymentLabels:
    app: slack-bot
  podSecurityContext:
    {}
  securityContext:
    {}
  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "2000Mi"
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Onyx Model Context Protocol (MCP) Server
# Allows LLMs to use Onyx like invoking tools or accessing resources
mcpServer:
  enabled: false  # Disabled by default
  replicaCount: 1
  image:
    repository: onyxdotapp/onyx-backend
    tag: ""  # Overrides the image tag whose default is the chart appVersion.
  # CORS origins for MCP clients (comma-separated)
  # Example: "https://claude.ai,https://app.cursor.sh"
  corsOrigins: ""
  podAnnotations: {}
  podLabels:
    scope: onyx-backend
  deploymentLabels:
    app: mcp-server
  containerPorts:
    server: 8090
  service:
    type: ClusterIP
    servicePort: 8090
    targetPort: mcp-server-port
    portName: mcp-server-port
  podSecurityContext: {}
  securityContext: {}
  resources:
    requests:
      cpu: "250m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"
  livenessProbe:
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 3
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  volumes: []
  volumeMounts: []
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_docfetching:
  replicaCount: 1
  logLevel: INFO
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 20
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
  deploymentLabels:
    app: celery-worker-docfetching
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 16Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

######################################################################
#
# End background workers section
#
######################################################################

redis:
  enabled: true
  redisStandalone:
    image: quay.io/opstree/redis
    tag: v7.0.15
    imagePullPolicy: IfNotPresent
    serviceType: ClusterIP
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
    # Use existing secret for Redis password
    redisSecret:
      secretName: onyx-redis
      secretKey: redis_password
  # Redis configuration
  externalConfig:
    enabled: true
    data: |
      appendonly no
      save ""
      maxmemory 400mb
      maxmemory-policy allkeys-lru
      timeout 0
      tcp-keepalive 300
  storageSpec:
    volumeClaimTemplate:
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 1Gi

minio:
  enabled: true
  mode: standalone
  replicas: 1
  drivesPerNode: 1
  existingSecret: onyx-objectstorage
  buckets:
    - name: onyx-file-store-bucket
  persistence:
    enabled: true
    size: 30Gi
    storageClass: ""
  service:
    type: ClusterIP
    port: 9000
  consoleService:
    type: ClusterIP
    port: 9001
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Code Interpreter - Python code execution service (beta feature)
codeInterpreter:
  enabled: true

  replicaCount: 1

  image:
    repository: onyxdotapp/code-interpreter
    pullPolicy: Always
    tag: ""  # Empty uses chart appVersion

  # Service configuration
  service:
    type: ClusterIP
    port: 8000
    targetPort: 8000

  # Execution limits
  codeInterpreter:
    maxExecTimeoutMs: 60000
    maxOutputBytes: "1000000"
    cpuTimeLimitSec: 5
    memoryLimitMb: 256
    host: "0.0.0.0"
    port: 8000

    # Kubernetes executor configuration (creates pods for code execution)
    kubernetesExecutor:
      namespace: ""  # Empty = same namespace as release
      image: ""  # Empty = default sandbox image
      serviceAccount: ""
      podResources:
        limits:
          cpu: "1"
          memory: "256Mi"
        requests:
          cpu: "100m"
          memory: "64Mi"

  # API container resources
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 1000m
      memory: 512Mi

  # RBAC for pod management (required for kubernetes executor)
  rbac:
    create: true

  # Security context
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000

  securityContext:
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: false
    runAsNonRoot: true
    runAsUser: 1000
    allowPrivilegeEscalation: false

  # Health probes
  livenessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 10
    periodSeconds: 10

  readinessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 5
    periodSeconds: 5

  nodeSelector: {}
  tolerations: []
  affinity: {}

  # Optional features
  ingress:
    enabled: false
  networkPolicy:
    enabled: false
  serviceAccount:
    create: true
    automount: true

ingress:
  enabled: false
  className: ""
  api:
    host: onyx.local
  webserver:
    host: onyx.local

letsencrypt:
  enabled: false
  email: "abc@abc.com"

# -- External PostgreSQL configuration. Use this when connecting to a pre-existing
# PostgreSQL instance (e.g., AWS RDS, GCP Cloud SQL, Azure Database for PostgreSQL).
# When enabled, set postgresql.enabled=false to skip deploying the bundled CloudNativePG cluster.
externalPostgresql:
  enabled: false
  # -- Hostname or Kubernetes service name of the external PostgreSQL instance.
  # Required when enabled=true.
  host: ""
  # -- Port for the external PostgreSQL instance.
  port: "5432"
  # -- Name of the database to connect to.
  database: "postgres"

# -- Governs all Secrets created or used by this chart. Values set by this chart will be base64 encoded in the k8s cluster.
auth:
  postgresql:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: true
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-postgresql'
    # -- Use a secret specified elsewhere.
    # When connecting to an external PostgreSQL instance, set this to the name of a pre-existing
    # K8s Secret that contains the database credentials. Also update secretKeys below to match
    # the key names used in that secret (they may differ from the CloudNativePG defaults).
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var.
    # CloudNativePG uses `username` and `password` as key names (the defaults below).
    # For external secrets, update these values to match your secret's actual key names.
    secretKeys:
      POSTGRES_USER: username
      POSTGRES_PASSWORD: password
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      username: "postgres"
      password: "postgres"
  redis:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: true
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-redis'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      REDIS_PASSWORD: redis_password
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      redis_password: "password"
  objectstorage:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: true
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-objectstorage'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      S3_AWS_ACCESS_KEY_ID: s3_aws_access_key_id
      S3_AWS_SECRET_ACCESS_KEY: s3_aws_secret_access_key
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      s3_aws_access_key_id: "minioadmin"
      s3_aws_secret_access_key: "minioadmin"
      rootUser: "minioadmin"
      rootPassword: "minioadmin"
  oauth:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: false
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-oauth'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      OAUTH_CLIENT_ID: "oauth_client_id"
      OAUTH_CLIENT_SECRET: "oauth_client_secret"
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      oauth_client_id: ""
      oauth_client_secret: ""
  smtp:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: false
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-smtp'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      SMTP_PASS: "smtp_pass"
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      smtp_pass: ""
  dbreadonly:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: false
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-dbreadonly'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      DB_READONLY_USER: db_readonly_user
      DB_READONLY_PASSWORD: db_readonly_password
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      db_readonly_user: ""
      db_readonly_password: ""
  opensearch:
    # Enable or disable this secret entirely. Will remove from env var
    # configurations and remove any created secrets.
    # Set to true when opensearch.enabled is true.
    enabled: false
    # Overwrite the default secret name, ignored if existingSecret is defined.
    secretName: 'onyx-opensearch'
    # Use a secret specified elsewhere.
    existingSecret: ""
    # This defines the env var to secret map, key is always upper-cased as an
    # env var.
    secretKeys:
      OPENSEARCH_ADMIN_USERNAME: opensearch_admin_username
      OPENSEARCH_ADMIN_PASSWORD: opensearch_admin_password
    # Secrets values IF existingSecret is empty. Key here must match the value
    # in secretKeys to be used. Values will be base64 encoded in the k8s
    # cluster.
    # Password must meet OpenSearch complexity requirements:
    # min 8 chars, uppercase, lowercase, digit, and special character.
    # CHANGE THIS FOR PRODUCTION.
    values:
      opensearch_admin_username: "admin"
      opensearch_admin_password: "OnyxDev1!"

configMap:
  # Change this for production uses unless Onyx is only accessible behind VPN
  AUTH_TYPE: "disabled"
  # 1 Day Default
  SESSION_EXPIRE_TIME_SECONDS: "86400"
  # Can be something like onyx.app, as an extra double-check
  VALID_EMAIL_DOMAINS: ""
  # For sending verification emails, true or false
  REQUIRE_EMAIL_VERIFICATION: ""
  # If unspecified then defaults to 'smtp.gmail.com'
  SMTP_SERVER: ""
  # For sending verification emails, if unspecified then defaults to '587'
  SMTP_PORT: ""
# 'your-email@company.com'
  SMTP_USER: ""
  # 'your-gmail-password'
  # SMTP_PASS: ""
  # 'your-email@company.com' SMTP_USER missing used instead
  EMAIL_FROM: ""
  # MinIO/S3 Configuration override
  S3_ENDPOINT_URL: ""  # only used if minio is not enabled
  S3_FILE_STORE_BUCKET_NAME: ""
  # Gen AI Settings
  GEN_AI_MAX_TOKENS: ""
  LLM_SOCKET_READ_TIMEOUT: "60"
  MAX_CHUNKS_FED_TO_CHAT: ""
  # Query Options
  DOC_TIME_DECAY: ""
  HYBRID_ALPHA: ""
  EDIT_KEYWORD_QUERY: ""
  # Don't change the NLP models unless you know what you're doing
  EMBEDDING_BATCH_SIZE: ""
  DOCUMENT_ENCODER_MODEL: ""
  NORMALIZE_EMBEDDINGS: ""
  ASYM_QUERY_PREFIX: ""
  ASYM_PASSAGE_PREFIX: ""
  DISABLE_RERANK_FOR_STREAMING: ""
  MODEL_SERVER_PORT: ""
  MIN_THREADS_ML_MODELS: ""
  # Indexing Configs
  VESPA_SEARCHER_THREADS: ""
  NUM_INDEXING_WORKERS: ""
  DISABLE_INDEX_UPDATE_ON_SWAP: ""
  DASK_JOB_CLIENT_ENABLED: ""
  CONTINUE_ON_CONNECTOR_FAILURE: ""
  EXPERIMENTAL_CHECKPOINTING_ENABLED: ""
  CONFLUENCE_CONNECTOR_LABELS_TO_SKIP: ""
  JIRA_CLOUD_API_VERSION: ""
  JIRA_SERVER_API_VERSION: ""
  GONG_CONNECTOR_START_TIME: ""
  NOTION_CONNECTOR_ENABLE_RECURSIVE_PAGE_LOOKUP: ""
  # Worker Parallelism
  CELERY_WORKER_DOCPROCESSING_CONCURRENCY: ""
  CELERY_WORKER_LIGHT_CONCURRENCY: ""
  CELERY_WORKER_LIGHT_PREFETCH_MULTIPLIER: ""
  CELERY_WORKER_USER_FILE_PROCESSING_CONCURRENCY: ""
  # OnyxBot SlackBot Configs
  ONYX_BOT_DISABLE_DOCS_ONLY_ANSWER: ""
  ONYX_BOT_DISPLAY_ERROR_MSGS: ""
  ONYX_BOT_RESPOND_EVERY_CHANNEL: ""
  NOTIFY_SLACKBOT_NO_ANSWER: ""
  DISCORD_BOT_TOKEN: ""
  DISCORD_BOT_INVOKE_CHAR: ""
  # Logging
  # Optional Telemetry, please keep it on (nothing sensitive is collected)? <3
  DISABLE_TELEMETRY: ""
  LOG_LEVEL: ""
  LOG_ALL_MODEL_INTERACTIONS: ""
  LOG_ONYX_MODEL_INTERACTIONS: ""
  LOG_VESPA_TIMING_INFORMATION: ""
  # Shared or Non-backend Related
  WEB_DOMAIN: "http://localhost:3000"
  # DOMAIN used by nginx
  DOMAIN: "localhost"
  # Chat Configs
  HARD_DELETE_CHATS: ""
  # User File Upload Configuration
  # Skip the token count threshold check (100,000 tokens) for uploaded files
  # For self-hosted: set to true to skip for all users
  SKIP_USERFILE_THRESHOLD: ""
  # For multi-tenant: comma-separated list of tenant IDs to skip threshold
  SKIP_USERFILE_THRESHOLD_TENANT_IDS: ""
